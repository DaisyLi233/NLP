{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abd94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 17:27:15.866395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/cn/0dflk63d5lj2brskz94j7c880000gn/T/ipykernel_39507/2890072755.py:24: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"youtoxic_english_1000.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"reihanenamdari/youtube-toxicity-data\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc96c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\nDont you reckon them 'black lives matter' ba...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\nDont you reckon them 'black lives matter' ba...     True       True   \n",
       "3  There are a very large number of people who do...    False      False   \n",
       "4  The Arab dude is absolutely right, he should h...    False      False   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0     False          False      False         False     False          False   \n",
       "1     False          False      False         False     False          False   \n",
       "2     False          False       True         False     False          False   \n",
       "3     False          False      False         False     False          False   \n",
       "4     False          False      False         False     False          False   \n",
       "\n",
       "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0     False         False            False         False  \n",
       "1     False         False            False         False  \n",
       "2     False         False            False         False  \n",
       "3     False         False            False         False  \n",
       "4     False         False            False         False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4be6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant columns\n",
    "df = df[['Text', 'IsToxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da0b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'IsToxic' to integer type\n",
    "df['IsToxic'] = df['IsToxic'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96869617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning for deep learning\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)                # remove URLs\n",
    "    t = re.sub(r\"@\\w+\", \"\", t)                   # remove @username\n",
    "    t = re.sub(r\"[^a-z0-9\\s!?']\", \" \", t)        # keep letters, digits, !, ?, '\n",
    "    t = re.sub(r\"\\s+\", \" \", t)                   # collapse multiple spaces\n",
    "    return t.strip()\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a45c777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 150, 150)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['Text'].values\n",
    "y = df['IsToxic'].values.astype(int)\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f658c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 5000\n",
    "MAX_LEN = 80\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text → sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq   = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_val_pad   = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_test_pad  = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c68401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors: 400000\n"
     ]
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "EMBED_DIM = 100\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "print(\"Loaded word vectors:\", len(embedding_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE, EMBED_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        vec = embedding_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff6214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee/anaconda3/envs/env_3/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │       \u001b[38;5;34m500,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> (1.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m500,000\u001b[0m (1.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "\n",
    "    layers.Embedding(\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        output_dim=EMBED_DIM,\n",
    "        weights=[embedding_matrix], # pre-trained embeddings\n",
    "        input_length=MAX_LEN,\n",
    "        trainable=False             # small dataset necessity (otherwise ruins pre-trained vectors)\n",
    "    ),\n",
    "\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88834b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5486 - loss: 0.7277 - val_accuracy: 0.6067 - val_loss: 0.6806\n",
      "Epoch 2/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6571 - loss: 0.6302 - val_accuracy: 0.6667 - val_loss: 0.6424\n",
      "Epoch 3/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7571 - loss: 0.5265 - val_accuracy: 0.6533 - val_loss: 0.6363\n",
      "Epoch 4/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8343 - loss: 0.4143 - val_accuracy: 0.6933 - val_loss: 0.5983\n",
      "Epoch 5/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8957 - loss: 0.3148 - val_accuracy: 0.6667 - val_loss: 0.5913\n",
      "Epoch 6/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9471 - loss: 0.2195 - val_accuracy: 0.6600 - val_loss: 0.6453\n",
      "Epoch 7/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9700 - loss: 0.1463 - val_accuracy: 0.6733 - val_loss: 0.6493\n",
      "Epoch 8/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9871 - loss: 0.0987 - val_accuracy: 0.6800 - val_loss: 0.6710\n",
      "Epoch 9/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9943 - loss: 0.0575 - val_accuracy: 0.7000 - val_loss: 0.6499\n",
      "Epoch 10/20\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9943 - loss: 0.0434 - val_accuracy: 0.6733 - val_loss: 0.6811\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b454b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Loss: 0.2206\n",
      "Final Train Accuracy: 0.9700\n",
      "Final Test Loss: 0.5865\n",
      "Final Test Accuracy: 0.6933\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6923    0.7778    0.7326        81\n",
      "           1     0.6949    0.5942    0.6406        69\n",
      "\n",
      "    accuracy                         0.6933       150\n",
      "   macro avg     0.6936    0.6860    0.6866       150\n",
      "weighted avg     0.6935    0.6933    0.6903       150\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63 18]\n",
      " [28 41]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Final training accuracy\n",
    "train_loss, train_acc = model.evaluate(X_train_pad, y_train, verbose=0)\n",
    "\n",
    "print(f\"Final Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Final Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# 2. Final test accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 3. Predictions\n",
    "y_pred_prob = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# 4. Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# 5. Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
